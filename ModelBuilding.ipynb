{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f6fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7ba976b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.debug.maxToStringFields\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create a Spark session\u001b[39;00m\n\u001b[1;32m      4\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mML Pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8g\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.shuffle.partitions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ML Pipeline\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Google Cloud Storage path\n",
    "gcs_path = 'gs://my-bigdatatech-project-jl/cleaned/Data_Cleaned.parquet'\n",
    "\n",
    "# Read the Parquet file from GCS\n",
    "df_spark = spark.read.parquet(gcs_path)\n",
    "\n",
    "# Show the DataFrame schema and the first few rows\n",
    "df_spark.printSchema()\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d7441",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Filter the Spark DataFrame (example condition)\n",
    "filtered_df = df_spark.filter(df_spark.totalFare.isNotNull() & df_spark.segmentsEquipmentDescription.isNotNull())\n",
    "\n",
    "# Limit to 10 rows\n",
    "limited_df = filtered_df.limit(50)\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "pandas_df = limited_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a73589",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_spark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ee66f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# convert date columns to actual date data type\n",
    "df_spark = df_spark.withColumn('searchDate', to_date(df_spark.searchDate, 'yyyy-MM-dd'))\n",
    "df_spark = df_spark.withColumn('flightDate', to_date(df_spark.flightDate, 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e6b65",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# get year-month variable \n",
    "df_spark = df_spark.withColumn(\"searchdate_yearmonth\", date_format(col(\"searchDate\"), \"yyyy-MM\"))\n",
    "df_spark = df_spark.withColumn('flightdate_yearmonth', date_format(col(\"flightDate\"), \"yyyy-MM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60972d65",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import dayofweek\n",
    "\n",
    "# get day of week as a number \n",
    "df_spark = df_spark.withColumn(\"searchDate_dayofweek\", dayofweek(col(\"searchDate\")) )\n",
    "df_spark = df_spark.withColumn(\"flightDate_dayofweek\", dayofweek(col(\"flightDate\")) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5fb8c6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# get day of week as name e.g. Monday\n",
    "df_spark = df_spark.withColumn(\"searchDate_dayofweekname\", date_format(col(\"searchDate\"), \"E\"))\n",
    "df_spark = df_spark.withColumn(\"flightDate_dayofweekname\", date_format(col(\"flightDate\"), \"E\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b1995",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# check if search or flight date falls on a weekend\n",
    "df_spark = df_spark.withColumn(\"searchDate_weekend\", when(df_spark.searchDate_dayofweek == 1, 1.0).when(df_spark.searchDate_dayofweek == 7, 1.0).otherwise(0))\n",
    "# check if search or flight date falls on a weekend\n",
    "df_spark = df_spark.withColumn(\"flightDate_weekend\", when(df_spark.flightDate_dayofweek == 1, 1.0).when(df_spark.flightDate_dayofweek == 7, 1.0).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d87a45",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Define a list of holiday dates\n",
    "holidays = [\n",
    "    '2024-01-01',  # New Year's Day\n",
    "    '2024-07-04',  # Independence Day\n",
    "    '2024-12-25',  # Christmas Day\n",
    "    '2024-11-28' # Thanksgiving Day\n",
    "]\n",
    "\n",
    "# Add a column to indicate if the flightDate is a holiday\n",
    "df_spark = df_spark.withColumn(\n",
    "    \"isHoliday\", \n",
    "    when(col(\"flightDate\").cast(\"string\").isin(holidays), 1).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9feb65",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Holiday Indicator\n",
    "df_spark = df_spark.withColumn(\"isHoliday\", when(col(\"flightDate\").cast(\"string\").isin(holidays), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901deaa5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import month\n",
    "\n",
    "# extract the month from flightDate and create a new column flightMonth\n",
    "df_spark = df_spark.withColumn(\"flightMonth\", month(col(\"flightDate\")))\n",
    "\n",
    "# create the season column based on the month\n",
    "df_spark = df_spark.withColumn(\n",
    "    \"season\",\n",
    "    when(col(\"flightMonth\").isin(6, 7, 8), \"Summer\")\n",
    "    .when(col(\"flightMonth\").isin(12, 1, 2), \"Winter\")\n",
    "    .when(col(\"flightMonth\").isin(3, 4, 5), \"Spring\")\n",
    "    .otherwise(\"Fall\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1aac5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import lit, datediff\n",
    "\n",
    "# 5. Flight Timing\n",
    "df_spark = df_spark.withColumn(\"flightTiming\", when((col(\"segmentsDepartureTimeRaw\") >= lit(\"18:00:00\")) &\n",
    "                                            (col(\"segmentsArrivalTimeRaw\") <= lit(\"06:00:00\")), \"Overnight\")\n",
    "                                       .otherwise(\"Daytime\"))\n",
    "\n",
    "# 6. Proximity of Booking\n",
    "df_spark = df_spark.withColumn(\"daysUntilFlight\", datediff(\"flightDate\", \"searchDate\")) \\\n",
    "           .withColumn(\"bookingProximity\", when(col(\"daysUntilFlight\") <= 1, \"Last Minute\")\n",
    "                                           .when((col(\"daysUntilFlight\") > 1) & (col(\"daysUntilFlight\") <= 7), \"Within a Week\")\n",
    "                                           .otherwise(\"Planned in Advance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b843da4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# check transformations\n",
    "df_spark.select(\"flightTiming\", \"daysUntilFlight\", \"bookingProximity\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff907b4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Converting boolean columns to integer - 0 or 1 - in PySpark\n",
    "df_spark = df_spark.withColumn('isBasicEconomy', col('isBasicEconomy').cast('int'))\n",
    "df_spark = df_spark.withColumn('isRefundable', col('isRefundable').cast('int'))\n",
    "df_spark = df_spark.withColumn('isNonStop', col('isNonStop').cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c1784",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_spark.select(\"isNonStop\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae5460a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# define seat availability volumes\n",
    "df_spark = df_spark.withColumn(\n",
    "    'seatAvailabilityCategory',\n",
    "    when(col('seatsRemaining') <= 20, 'low')\n",
    "    .when((col('seatsRemaining') > 20) & (col('seatsRemaining') <= 100), 'medium')\n",
    "    .otherwise('high')\n",
    ")\n",
    "\n",
    "df_spark.select('seatsRemaining', 'seatAvailabilityCategory').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7088dcf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# correct columns with right data types\n",
    "\n",
    "df_spark = df_spark.withColumn(\"travelDuration\", col(\"travelDuration\").cast(\"float\")) \\\n",
    "                   .withColumn(\"segmentsDurationInSeconds\", col(\"segmentsDurationInSeconds\").cast(\"int\")) \\\n",
    "                   .withColumn(\"segmentsDistance\", col(\"segmentsDistance\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfaaf39",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# ensure columns are numeric\n",
    "df_spark = df_spark.withColumn(\"elapsedDays\", col(\"elapsedDays\").cast(\"int\")) \\\n",
    "           .withColumn(\"isBasicEconomy\", col(\"isBasicEconomy\").cast(\"int\")) \\\n",
    "           .withColumn(\"isRefundable\", col(\"isRefundable\").cast(\"int\")) \\\n",
    "           .withColumn(\"isNonStop\", col(\"isNonStop\").cast(\"int\")) \\\n",
    "           .withColumn(\"baseFare\", col(\"baseFare\").cast(\"float\")) \\\n",
    "           .withColumn(\"seatsRemaining\", col(\"seatsRemaining\").cast(\"int\")) \\\n",
    "           .withColumn(\"totalTravelDistance\", col(\"totalTravelDistance\").cast(\"int\")) \\\n",
    "           .withColumn(\"segmentsDurationInSeconds\", col(\"segmentsDurationInSeconds\").cast(\"int\")) \\\n",
    "           .withColumn(\"segmentsDistance\", col(\"segmentsDistance\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb87a87",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Outlier detection for 'totalFare' using IQR\n",
    "quantiles = df_spark.approxQuantile(\"totalFare\", [0.25, 0.75], 0.01)\n",
    "Q1, Q3 = quantiles\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out outliers\n",
    "df_spark = df_spark.filter((col(\"totalFare\") >= lower_bound) & (col(\"totalFare\") <= upper_bound))\n",
    "\n",
    "# Display summary statistics for verification\n",
    "df_spark.select(\"totalFare\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4589f998",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType, FloatType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656bee3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"legId\", StringType(), True),\n",
    "    StructField(\"searchDate\", DateType(), True),\n",
    "    StructField(\"flightDate\", DateType(), True),\n",
    "    StructField(\"startingAirport\", StringType(), True),\n",
    "    StructField(\"destinationAirport\", StringType(), True),\n",
    "    StructField(\"fareBasisCode\", StringType(), True),\n",
    "    StructField(\"travelDuration\", StringType(), True),\n",
    "    StructField(\"elapsedDays\", IntegerType(), True),\n",
    "    StructField(\"isBasicEconomy\", IntegerType(), True),\n",
    "    StructField(\"isRefundable\", IntegerType(), True),\n",
    "    StructField(\"isNonStop\", IntegerType(), True),\n",
    "    StructField(\"baseFare\", FloatType(), True),\n",
    "    StructField(\"totalFare\", FloatType(), True),\n",
    "    StructField(\"seatsRemaining\", IntegerType(), True),\n",
    "    StructField(\"totalTravelDistance\", IntegerType(), True),\n",
    "    StructField(\"segmentsDepartureTimeEpochSeconds\", LongType(), True),\n",
    "    StructField(\"segmentsArrivalTimeEpochSeconds\", LongType(), True),\n",
    "    StructField(\"segmentsDurationInSeconds\", IntegerType(), True),\n",
    "    StructField(\"segmentsDistance\", IntegerType(), True),\n",
    "    StructField(\"segmentsCabinCode\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8bbc0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import isnan\n",
    "\n",
    "sampled_df = df_spark.sample(fraction=0.01, seed=42)\n",
    "\n",
    "# split data into training and testing sets\n",
    "train_data, test_data = sampled_df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# remove nan values\n",
    "train_data = train_data.filter(~isnan(\"totalFare\"))\n",
    "test_data = test_data.filter(~isnan(\"totalFare\"))\n",
    "\n",
    "# features for the model\n",
    "feature_columns = ['isRefundable', 'isBasicEconomy', 'isNonStop', 'searchDate_weekend', \n",
    "                   'flightDate_weekend', 'isHoliday', 'season', 'flightTiming', 'bookingProximity']\n",
    "\n",
    "feature_columns = [\n",
    "    col for col in feature_columns if train_data.select(col).distinct().count() > 1\n",
    "]\n",
    "\n",
    "numeric_features = [col for col in feature_columns if train_data.schema[col].dataType.typeName() != 'string']\n",
    "\n",
    "# index and encode categorical features\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col + \"Index\") for col in feature_columns if col not in numeric_features\n",
    "]\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=col + \"Index\", outputCol=col + \"Vector\") for col in feature_columns if col not in numeric_features\n",
    "]\n",
    "\n",
    "# assembling features into a vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[col + \"Vector\" for col in feature_columns if col not in numeric_features] + numeric_features,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# linear Regression model\n",
    "lr = LinearRegression(labelCol=\"totalFare\")\n",
    "\n",
    "# create Pipeline \n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, lr])\n",
    "\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.1, 0.01, 0.001]) \n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])  \n",
    "    .addGrid(lr.maxIter, [10, 50, 100])  \n",
    "    .build()\n",
    ")\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"totalFare\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,  \n",
    "    estimatorParamMaps=paramGrid,  \n",
    "    evaluator=evaluator, \n",
    "    numFolds=3, \n",
    "    parallelism=2 \n",
    ")\n",
    "\n",
    "cv_model = crossval.fit(train_data)\n",
    "\n",
    "# make predictions\n",
    "predictions = cv_model.transform(test_data)\n",
    "\n",
    "predictions = predictions.filter(~isnan(\"prediction\") & ~isnan(\"totalFare\"))\n",
    "\n",
    "# evaluate the model\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: 'r2'})\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "\n",
    "predictions.select(\"flightDate\", \"totalFare\", \"prediction\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee718d8d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# transform the training data\n",
    "transformed_train_data = model.transform(train_data)\n",
    "\n",
    "# save the transformed data to the /trusted folder\n",
    "transformed_train_data.write.parquet(\"gs://my-bigdatatech-project-jl/trusted/processed_data\", mode='overwrite')\n",
    "\n",
    "print('transformed training data saved')\n",
    "\n",
    "# save the LR model to the /models folder\n",
    "model.save(\"gs://my-bigdatatech-project-jl/models/linear_regression_model\")\n",
    "\n",
    "print('LR model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e7c11d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
